1.  RNN，没办法做并行，每个下一步都要前面的中间结果； 传统word2vec, 无法解决一词多义的问题，与训练好的向量就永久不变了
2. self.attention 机制来进行并行计算，在输入输出都相同； self.attention来决定哪个重要， 考虑当前词的上下文语境
3.  BERT $$Z = softmax(Q*K^T/sqrt(d_k))*V$$ 第N个词在这个序列中于每一个的关系，每一次的输入序列不一样，不同上下文；矩阵惩罚的计算；并行加速
4.  Multi-head机制 Q，K，V 多头，得到多个特征表达，将所有特征拼接在一起，可以通过一层全连接来降维；输入输出都是向量，那么可以多层
5.  位置信息表达，加了positional encoding(余弦正弦周期表达;归一化： BN：让均值为0，标准差为1；LN：训练速度快更稳定。  连接：基本的残差链接，
加一个映射，至少不会比原来差
6.  decoder中有encoder-decoder attention(编码器KV）和self-attention； 加入mask机制，一个接着一个往后出答案，把需要预测的值遮挡到；得到最终输出结果，损失函数cross-
7.  BERT是transformer的encoder部分，得到的是向量的表示形式，不需要标签有预料就可以了
8.  【SEP】两个橘子之间的连接符，【CLS】表示要做分类的向量
9.  使用BERT  i.e.阅读理解题，输入文章和问题输出为理解答案的位置； 需要分别计算答案的启示和终止位置，单独对end索引做训练，end的特征向量对文章中的每个词做dot product(内积）softmax得到概率最大的为end=====训练出编码器
10. 
