词频影响了词向量的空间分布，我们希望句子的embedding表达可以用来度量句子间的相似性，但如果单词的embedding的分布受词频影响，这种相似性不太能让人信服。
BERT会得到一个各向异性的词嵌入空间，也就是词向量分布不均匀。如果所有单词的加和正好落在语义不明确的地方，相似的度量准确度就会很低。
BERT输出会很容易定义在这些区域这样他们的cosine相似度（空间相似度）与语义相似度就不在一致了。
针对这一问题，首先我们在BERT编码器的基础上引入了句法树编码器（structure encoding）
,我们将两者concatenate得到S编码。这在一定程度上缓解了BERT的各向异性问题，在此基础上我们采用了孪生网络的方法，在一个batch中随机采样找到一个正样本对应的五个负样本。
分别对其进行编码。在正样本网络中我们用了label smoothing来归一化yls= (1−α)∗oh+α/k。
分类的概率对于单个标签的用的是softmax,我们使用的损失函数是交叉熵。对于正负网络我们共享了权重，相当于增加了辅助任务使语义相似的句子区分的更加明显
，在此我们认为可以用了cosine similarity来计算损失。loss是对same batch的每个负样本与正样本计算cosine similarity的总和的均值。 
余弦值越接近0，就表明夹角越接近90度，也就是两个向量越不相似，这就叫"余弦相似性"。
最后我们用了L2正则来预防过拟合问题。总和的损失是他们三个的相加。
我们用的batch size为（16.32）optimizer区分于BERT的Admm weight decay{0.999,0.9}
对于测试集我们单个标签用的是accuracy多个标签用的是exact match来得到我们的模型性能指标。

BERT自监督模型
1.  RNN，没办法做并行，每个下一步都要前面的中间结果； 传统word2vec, 无法解决一词多义的问题，与训练好的向量就永久不变了； Encoder端可以并行计算，一次性将输入序列全部encoding出来，但Decoder端不是一次性把所有单词(token)预测出来的。对于每个序列，selfattention可以计算出，x_i，x_j的点乘结果。
2. self.attention 机制来进行并行计算，在输入输出都相同； self.attention来决定哪个重要， 考虑当前词的上下文语境
3.  BERT $$Z = softmax(Q*K^T/sqrt(d_k))*V$$ 第N个词在这个序列中于每一个的关系，每一次的输入序列不一样，不同上下文；矩阵惩罚的计算；并行加速
4.  Multi-head机制 Q，K，V 多头，得到多个特征表达，将所有特征拼接在一起，可以通过一层全连接来降维；输入输出都是向量，那么可以多层
5.  位置信息表达，加了positional encoding(余弦正弦周期表达;归一化： BN：让均值为0，标准差为1；LN：训练速度快更稳定。  连接：基本的残差链接，
加一个映射，至少不会比原来差
6.  decoder中有encoder-decoder attention(编码器KV）和self-attention； 加入mask机制，一个接着一个往后出答案，把需要预测的值遮挡到；得到最终输出结果，损失函数cross-
7.  BERT是transformer的encoder部分，得到的是向量的表示形式，不需要标签有预料就可以了
8.  【SEP】两个橘子之间的连接符，【CLS】表示要做分类的向量
9.  使用BERT  i.e.阅读理解题，输入文章和问题输出为理解答案的位置； 需要分别计算答案的启示和终止位置，单独对end索引做训练，end的特征向量对文章中的每个词做dot product(内积）softmax得到概率最大的为end=====训练出编码器
10. q，k，v分别是query，key，value，对于encoder self-attention，第一次计算的初始值是每个字的embedding，q用来和k做点乘计算相似度，这些相似度经过softmax变成权重，然后权重和v相乘，其实就是v的一个加权平均。如果是encoder-decoder attention，q是decoder的hidden state，k和v是encoder各个位置的hidden state。
