1.RNN，没办法做并行，每个下一步都要前面的中间结果； 传统word2vec, 无法解决一词多义的问题，与训练好的向量就永久不变了
2. self.attention 机制来进行并行计算，在输入输出都相同； self.attention来决定哪个重要， 考虑当前词的上下文语境
3.  BERT $$Z = softmax(Q*K^T/sqrt(d_k))*V$$ 第N个词在这个序列中于每一个的关系，每一次的输入序列不一样，不同上下文；矩阵惩罚的计算；并行加速
4.  Multi-head机制 Q，K，V 多头，得到多个特征表达，将所有特征拼接在一起，可以通过一层全连接来降维；输入输出都是向量，那么可以多层
5.  位置信息表达，加了positional encoding(余弦正弦周期表达;归一化： BN：让均值为0，标准差为1；LN：训练速度快更稳定。  连接：基本的残差链接，
加一个映射，至少不会比原来差
