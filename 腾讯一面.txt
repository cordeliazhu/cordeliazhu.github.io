3.4号天衍实验室一面
==========
##数据不平衡问题
focal loss，过采样增加负样本随机采样
##三元组实体构建
举一个例子
##编程问题
接雨水leetcode11，代码优化if __name__ == "__main__"。时间复杂度O(n)
##你后面想做什么方向
1.链接预测，实体链接细粒度问题解决inference
2.中文实体嵌套的问题


3.9号应用研究一面
==========
##tf-idf
TF（单词频率）关键字中某一个单词在目标文档中出现的作用；
idf（惩罚出现在太多文档中的单词）


##activation function
激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。
将神经网络非线性化，函数是连续且可导的；tanh的导数取值范围在0至1之间，优于sigmoid的0至1/4，在一定程度上，减轻了梯度消失的问题。
sigmoid和tanh比较

sigmoid在输入处于[-1,1]之间时，函数值变化敏感，一旦接近或者超出区间就失去敏感性，处于饱和状态，影响神经网络预测的精度值；
tanh的变化敏感区间较宽，导数值渐进于0、1，符合人脑神经饱和的规律，比sigmoid函数延迟了饱和期；
tanh在原点附近与y=x函数形式相近，当激活值较低时，可以直接进行矩阵运算，训练相对容易；
tanh和sigmoid都是全部激活（fire），使得神经网络较重（heavy）。
relu对比于sigmoid：

sigmoid的导数，只有在0附近，具有较好的激活性，而在正负饱和区的梯度都接近于0，会造成梯度弥散；而relu的导数，在大于0时，梯度为常数，不会导致梯度弥散。
relu函数在负半区的导数为0 ，当神经元激活值进入负半区，梯度就会为0，也就是说，这个神经元不会被训练，即稀疏性；
relu函数的导数计算更快，程序实现就是一个if-else语句；而sigmoid函数要进行浮点四则运算，涉及到除法；

##三扇门问题
一定要换门，第一扇门概率1/3，第三门2/3.因为第二个门打开了。



