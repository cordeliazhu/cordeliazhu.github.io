您好，我是佐治亚理工学院电子与计算机工程研一的学生，研一上半年主要学习了C++高级编程原理和计算机网络。11月份加入万科集团做NLP算法的实习，主要做文本分类及建筑行业知识图谱schema三元组数据方面的工作，并在实习阶段和指导我的老师发表了一片有关于语义相似的字句分类的文章，目前已经投稿至ACL。 这学期，我上的课是网络安全和统计机器学习。 我觉得我的优点是自学能力强，、在大学的时候也是自学通过了GRE的考试。

词频率越大的越靠近原点，频率越小越远离。会得到一个各向异性的词嵌入空间，也就是词向量分布不均匀。那低频词周围就会有很多空隙，BERT输出会很容易定义在这些区域这样他们的cosine相似度（空间相似度）与语义相似度就不在一致了。针对这一问题，首先我们在BERT编码器的基础上引入了句法树编码器（structure encoding）,我们将两者concatenate得到S编码。这在一定程度上缓解了BERT的各向异性问题，在此基础上我们采用了孪生网络的方法，在一个batch中随机采样找到一个正样本对应的五个负样本。分别对其进行编码。在正样本网络中我们用了label smoothing来归一化yls= (1−α)∗oh+α/k。分类的概率对于单个标签的用的是softmax,我们使用的损失函数是交叉熵。对于正负网络我们共享了权重，我们用了cosine similarity来计算损失。最后我们用了L2正则来预防过拟合问题。总和的损失是他们三个的相加。我们用的batch size为（16.32）optimizer区分于BERT的Admm weight decay{0.999,0.9}对于测试集我们单个标签用的是accuracy多个标签用的是exact match来得到我们的模型性能指标。