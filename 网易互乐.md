# 网易NLP一面
## 主要问项目问题
###用linux指令找到你最近最常用的五个指令


```C++
#grep命令
"""
该命令常用于分析一行的信息，若当中有我们所需要的信息，就将该行显示出来，该命令通常与管道命令一起使用，用于对一些命令的输出进行筛选加工等等tation of our code is shown in the video.
"""
# 把ls -l的输出中包含字母file（不区分大小写）的内容输出
ls -l | grep -i file;
```
###样本不均衡怎么办
<font color='yellow'>欠采样（undersampling）法</font>是去除训练集内一些多数样本，使得两类数据量级接近，然后在正常进行学习。

过采样（oversampling）是对训练集内的少数样本进行扩充，既增加少数样本使得两类数据数目接近，然后再进行学习。

阈值移动：这类方法的中心思想不是对样本集和做再平衡设置，而是对算法的决策过程进行改进。

###Git指令
git fetch: git fetch really only downloads new data from a remote repository - but it doesn't integrate any of this new data into your working files.


git pull: in contrast, is used with a different goal in mind: to update your current HEAD branch with the latest changes from the remote server. This means that pull not only downloads new data; it also directly integrates it into your current working copy files.

###Elmo和BERT的区别，为什么BERT好
ELmo （BiLSTM），上下文是硬拼接； 数据量有关

###LN和BN的区别
如果我们将一批文本组成一个batch，那么BN的操作方向是，对每个位置的词在batch维度分别操。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是针对每个位置进行缩放，这不符合NLP的规律

而LN则是针对一句话进行缩放的，且LN一般用在第三维度，如[batchsize, seq_len, dims]中的dims，一般为词向量的维度，或者是RNN的输出维度等等，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。


###简单的SQl语句

###我们的上线的东西
QoS
###文本分类的方法
1.简单的是正则表达式
2.难的事BiLSTM和CRF结合
###LSTM
###归一化和标准化
归一化
1）Min-Max Normalization
   $x' = (x - X_min) / (X_max - X_min)$

2）平均归一化
   $x' = (x - μ) / (MaxValue - MinValue)$
标准化
（1）Z-score规范化（标准差标准化 / 零均值标准化）
  x' = (x - μ)／σ
   （1）如果对输出结果范围有要求，用归一化。
  （2）如果数据较为稳定，不存在极端的最大最小值，用归一化。
  （3）如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。





