1. leetcode 曼哈顿距离1030
2. 对BERT的详细理解
只有BERT表征会基于所有层的左右两侧语境，自监督任务组成， 即MLM和NSP， 
3.MLM
15%的wordPiece Token会被随机mask掉，在训练模型的时候，一个句子会被多次喂到模型中进行阐述学习， 80%的时候直接替换为[mask], 10%的时候将其替换为其他任意单词, 10%的时候会保留原始token；被随机替换掉的概率只有15%*10% = 1.5%

4.NSP： 判断句子B是否句子A的下文
50%保留抽取的两句话，她们符合IsNext关系, 另外50%的第二句话是随机从预料中提取的，他们的关系是NotNext。 这个关系保存在【cls】符号中

5. BERT的输入编码向量（长度512）是word Piece Token
WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。例如图4的示例中‘playing’被拆分成了‘play’和‘ing’；
和Position embedding
置嵌入是指将单词的位置信息编码成特征向量，位置嵌入是向模型中引入单词位置关系的至关重要的一环。（PE（pos, 2i) = sin(pos/10000^(2i/d_model))   PE（pos, 2i + 1) = cos(pos/10000^(2i/d_model))
和分割嵌入（Segment Embedding）：用于区分两个句子，例如B是否是A的下文（对话场景，问答场景等）。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1。

6.BERT的损失函数是啥
cross entropy对分类， MSE对huigui
