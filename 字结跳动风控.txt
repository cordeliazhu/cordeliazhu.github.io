1. leetcode 曼哈顿距离1030
2. 对BERT的详细理解
只有BERT表征会基于所有层的左右两侧语境，自监督任务组成， 即MLM和NSP， 
3.MLM
15%的wordPiece Token会被随机mask掉，在训练模型的时候，一个句子会被多次喂到模型中进行阐述学习， 80%的时候直接替换为[mask], 10%的时候将其替换为其他任意单词, 10%的时候会保留原始token；被随机替换掉的概率只有15%*10% = 1.5%

4.NSP： 判断句子B是否句子A的下文
50%保留抽取的两句话，她们符合IsNext关系, 另外50%的第二句话是随机从预料中提取的，他们的关系是NotNext。 这个关系保存在【cls】符号中

5. BERT的输入编码向量（长度512）是word Piece Token
WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。例如图4的示例中‘playing’被拆分成了‘play’和‘ing’；
和Position embedding
置嵌入是指将单词的位置信息编码成特征向量，位置嵌入是向模型中引入单词位置关系的至关重要的一环。（PE（pos, 2i) = sin(pos/10000^(2i/d_model))   PE（pos, 2i + 1) = cos(pos/10000^(2i/d_model))
和分割嵌入（Segment Embedding）：用于区分两个句子，例如B是否是A的下文（对话场景，问答场景等）。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1。

6.BERT的损失函数是啥
cross entropy对分类， MSE对huigui

#################二面
1. Python的全局局部
	class variable：
		def __init__(self, a):
			self.a = "我是类变量"
		def showvariable(self):
			self.b = "我函数变量"
			print(self.a)
			print(b)
	variable(1).showvariable()
	如果名称相同，函数优先级将会使用局部变量
2. .copy()又忘记了 leetcode 46题
class Solution(object):
    def permute(self, num):
        self.res = []
        if len(num) < 1:
            return [[]]
        self.backtrack( num.copy(), 0)
        return self.res
    def backtrack(self, num, a):
        if (a == n):
            if num[:] not in self.res:
                self.res.append(num[:])
         
        for i in range(a,n):
            num[i],num[a] = num[a], num[i]
            self.backtrack(num, a+1)
            
            num[i],num[a] = num[a], num[i]
if __name__ == "__main__":
    nums = [1,1,2]
    n = len(nums)       
    S = Solution()       
    print(S.permute(nums))
3. 字节有毒
4. 全排列问题（就是回朔法）
